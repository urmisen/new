{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " NER_with_NLTK.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPLtpmi3Sfln"
      },
      "source": [
        "### NER with NLTK\n",
        "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use nltk to find the named entities in this article.\n",
        "\n",
        "What might the article be about, given the names you found?\n",
        "\n",
        "Along with nltk, sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported.\n",
        "\n",
        "* Tokenize article into sentences.\n",
        "* Tokenize each sentence in sentences into words using a list comprehension.\n",
        "* Inside a list comprehension, tag each tokenized sentence into parts of speech using nltk.pos_tag().\n",
        "* Chunk each tagged sentence into named-entity chunks using nltk.ne_chunk_sents(). Along with pos_sentences, specify the additional keyword argument binary=True.\n",
        "* Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute label, and if the chunk.label() is equal to \"NE\". If so, print that chunk.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTwG8ZQ9SabR"
      },
      "source": [
        "# Tokenize the article into sentences: sentences\n",
        "sentences = sent_tokenize(article)\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp46oF-zLskK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}