{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilayer_perceptron.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndhb_QwsOFSU",
        "outputId": "045f8e53-b3d0-490b-e607-e6cd3436dcdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow keras numpy matplotlib pandas sklearn biopython"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.6/dist-packages (1.78)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ts2EpTY1b8r"
      },
      "source": [
        "# **1. Initialize Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNB1CanOOGYX",
        "outputId": "a9e84c35-f2c2-4b4b-85db-f5ee286bead2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from Bio import SeqIO\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas import ExcelWriter\n",
        "from pandas import ExcelFile\n",
        "from random import seed\n",
        "from random import random\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import exp\n",
        "import re, ast\n",
        "\n",
        "\n",
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network\n",
        "\n",
        "seed(1)\n",
        "network = initialize_network(2, 1, 2)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
            "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ynnSgZW1Vlk"
      },
      "source": [
        "Running the example, you can see that the code prints out each layer one by one. You can see the hidden layer has one neuron with 2 input weights plus the bias. The output layer has 2 neurons, each with 1 weight plus the bias.\n",
        "\n",
        "# **2. Forward Propagate**\n",
        "We can break forward propagation down into three parts:\n",
        "\n",
        "* Neuron Activation.\n",
        "* Neuron Transfer.\n",
        "* Forward Propagation.\n",
        "\n",
        "### **2.1. Neuron Activation**\n",
        "**activation = sum(weight_i * input_i) + bias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73HVE6HweDXX"
      },
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGXS69Os6MGs"
      },
      "source": [
        "### **2.2. Neuron Transfer**\n",
        "Once a neuron is activated, we need to transfer the activation to see what the neuron output actually is.\n",
        "\n",
        "Different transfer functions can be used. It is traditional to use the sigmoid activation function, but you can also use the tanh (hyperbolic tangent) function to transfer outputs. More recently, the rectifier transfer function has been popular with large deep learning networks.\n",
        "\n",
        "The sigmoid activation function looks like an S shape, it’s also called the logistic function. It can take any input value and produce a number between 0 and 1 on an S-curve. It is also a function of which we can easily calculate the derivative (slope) that we will need later when backpropagating error.\n",
        "\n",
        "We can transfer an activation function using the sigmoid function as follows:\n",
        "\n",
        "### **output = 1 / (1 + e^(-activation))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKtX7nMi7SmX"
      },
      "source": [
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a7FlX3h9rvv"
      },
      "source": [
        "### **2.3. Forward Propagation**\n",
        "Forward propagating an input is straightforward.\n",
        "\n",
        "We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer.\n",
        "\n",
        "Below is a function named forward_propagate() that implements the forward propagation for a row of data from our dataset with our neural network.\n",
        "\n",
        "You can see that a neuron’s output value is stored in the neuron with the name ‘output‘. You can also see that we collect the outputs for a layer in an array named new_inputs that becomes the array inputs and is used as inputs for the following layer.\n",
        "\n",
        "The function returns the outputs from the last layer also called the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ypOX55W9h7A"
      },
      "source": [
        "\n",
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIPtvTP5-gjV"
      },
      "source": [
        "# **3. Back Propagate Error**\n",
        "\n",
        "The backpropagation algorithm is named for the way in which weights are trained.\n",
        "Error is calculated between the expected outputs and the outputs forward propagated from the network. These errors are then propagated backward through the network from the output layer to the hidden layer, assigning blame for the error and updating weights as they go.\n",
        "\n",
        "The math for backpropagating error is rooted in calculus, but we will remain high level in this section and focus on what is calculated and how rather than why the calculations take this particular form.\n",
        "\n",
        "This part is broken down into two sections.\n",
        "\n",
        "* Transfer Derivative.\n",
        "* Error Backpropagation.\n",
        "\n",
        "### **3.1. Transfer Derivative**\n",
        "Given an output value from a neuron, we need to calculate it’s slope.\n",
        "\n",
        "We are using the sigmoid transfer function, the derivative of which can be calculated as follows:\n",
        "\n",
        "**derivative = output * (1.0 - output)**\n",
        "Below is a function named transfer_derivative() that implements this equation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjcqUYvOGGbS"
      },
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzOxS5j3GQIb"
      },
      "source": [
        "### **3.2. Error Backpropagation**\n",
        "The first step is to calculate the error for each output neuron, this will give us our error signal (input) to propagate backwards through the network.\n",
        "\n",
        "The error for a given neuron can be calculated as follows:\n",
        "\n",
        "**error = (expected - output) * transfer_derivative(output)**\n",
        "\n",
        "The error signal for a neuron in the hidden layer is calculated as the weighted error of each neuron in the output layer. Think of the error traveling back along the weights of the output layer to the neurons in the hidden layer.\n",
        "\n",
        "**error = (weight_k * error_j) * transfer_derivative(output)**\n",
        "\n",
        "Where error_j is the error signal from the jth neuron in the output layer, weight_k is the weight that connects the kth neuron to the current neuron and output is the output for the current neuron.\n",
        "\n",
        "Below is a function named backward_propagate_error() that implements this procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGbYg9zyUQvP"
      },
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLm1OQB5UVPJ"
      },
      "source": [
        "You can see that the error signal calculated for each neuron is stored with the name ‘delta’. You can see that the layers of the network are iterated in reverse order, starting at the output and working backwards. This ensures that the neurons in the output layer have ‘delta’ values calculated first that neurons in the hidden layer can use in the subsequent iteration. I chose the name ‘delta’ to reflect the change the error implies on the neuron (e.g. the weight delta).\n",
        "\n",
        "You can see that the error signal for neurons in the hidden layer is accumulated from neurons in the output layer where the hidden neuron number j is also the index of the neuron’s weight in the output layer neuron[‘weights’][j]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIKibvlaUU7E",
        "outputId": "1b7a989e-66cd-4200-8a4e-551b03f0b152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test backpropagation of error\n",
        "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\t\t[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "expected = [0, 1]\n",
        "backward_propagate_error(network, expected)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n",
            "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHl2y8DoWfOn"
      },
      "source": [
        "# **4. Train Network**\n",
        "The network is trained using stochastic gradient descent.\n",
        "\n",
        "This involves multiple iterations of exposing a training dataset to the network and for each row of data forward propagating the inputs, backpropagating the error and updating the network weights.\n",
        "\n",
        "This part is broken down into two sections:\n",
        "\n",
        "* Update Weights.\n",
        "* Train Network.\n",
        "\n",
        "### **4.1. Update Weights**\n",
        "\n",
        "Once errors are calculated for each neuron in the network via the back propagation method above, they can be used to update weights.\n",
        "\n",
        "Network weights are updated as follows:\n",
        "\n",
        "**weight = weight + learning_rate * error * input**\n",
        "\n",
        "Where weight is a given weight, learning_rate is a parameter that you must specify, error is the error calculated by the backpropagation procedure for the neuron and input is the input value that caused the error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us4LfWmGadTw"
      },
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWBZDRgbasgj"
      },
      "source": [
        "### **4.2. Train Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlY6hkNPcOrs"
      },
      "source": [
        "\n",
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tsum_error = 0\n",
        "\t\tfor row in train:\n",
        "\t\t\toutputs = forward_propagate(network, row)\n",
        "\t\t\texpected = [0 for i in range(n_outputs)]\n",
        "\t\t\texpected[row[-1]] = 1\n",
        "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "\t\t\tbackward_propagate_error(network, expected)\n",
        "\t\t\tupdate_weights(network, row, l_rate)\n",
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6FOCDmWck8F"
      },
      "source": [
        "Below is the complete example. We will use 2 neurons in the hidden layer. It is a binary classification problem (2 classes) so there will be two neurons in the output layer. The network will be trained for 20 epochs with a learning rate of 0.5, which is high because we are training for so few iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-TLE-Npc3kY"
      },
      "source": [
        "# **5. Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq0zxD5llsIb"
      },
      "source": [
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "\toutputs = forward_propagate(network, row)\n",
        "\treturn outputs.index(max(outputs))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_gsyX5Kl8jw"
      },
      "source": [
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "\tn_inputs = len(train[0]) - 1\n",
        "\tn_outputs = len(set([row[-1] for row in train]))\n",
        "\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(network, row)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2o31-Ws13JF"
      },
      "source": [
        "# Python program to convert decimal to binary \n",
        "    \n",
        "# Function to convert Decimal number  \n",
        "# to Binary number  \n",
        "def decimalToBinary(n,n_bit):  \n",
        "    p=\"0\"+n_bit+\"b\"\n",
        "    res = str([int(i) for i in list(format(n,p))] )\n",
        "    return res  "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzPnS4WO7m1s"
      },
      "source": [
        "# Python3 program to convert  \n",
        "# list into a list of lists \n",
        "  \n",
        "def extractDigits(lst): \n",
        "    return [[el] for el in lst] "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ZizaZiOSHQ",
        "outputId": "895db0ce-b730-406f-eaf8-64e52391c33f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# Driver code  \n",
        "if __name__ == '__main__': \n",
        "\n",
        "  id=[]\n",
        "  bit = []\n",
        "  target=[]\n",
        "  dataset=[]\n",
        "  data=[]\n",
        "  #input the number of bits\n",
        "  n_bit=input(\"enter :\")\n",
        "  #calculate the total no of samples\n",
        "  n_total= 2** int(n_bit)\n",
        "  #taking the half of the tatal\n",
        "  n_half=int(n_total/2)\n",
        "  #decimal to binary\n",
        "  for i in range(n_total):\n",
        "    bin = decimalToBinary(i,n_bit)\n",
        "    bit.append(bin)\n",
        "    id.append(i)\n",
        "  #creating target(1) for first half\n",
        "  for j in range(0,n_half):\n",
        "    target.append(1)\n",
        "\n",
        "  #creating target(0) for second half\n",
        "  for k in range(n_half,n_total):\n",
        "    target.append(0)\n",
        "\n",
        "  #creating Dataframe\n",
        "  raw_data ={\n",
        "              'ID':id,\n",
        "              'Bit': bit,\n",
        "              'Target':target\n",
        "              }\n",
        "  df=pd.DataFrame(raw_data,columns=['ID','Bit','Target'])\n",
        "  is_even = df['ID'].astype(int) % 2 == 0\n",
        "\n",
        "  df_train = df[is_even]\n",
        "  df_test = df[~is_even]\n",
        "\n",
        "  print(\"Training Dataset :\\n\",df_train)\n",
        "  print(\"Testing Dataset :\\n\",df_test)\n",
        "  \n",
        "\n",
        "  #loading train data\n",
        "  X=df_train.Bit.values.tolist()\n",
        "  Y=df_train.Target.values.tolist()\n",
        "  #convert the list of strings to list of list\n",
        "  data = [list(map(int,k.strip(\"[]\").split(\", \"))) for k in X]\n",
        "  #convertthe list of integer to list of list\n",
        "  t= extractDigits(Y)\n",
        "  #merger elementwise the data and corresponding target\n",
        "  dataset= [i + j for i, j in zip(data, t)] \n",
        "  print(\"TrainSet :\\n\",dataset)\n",
        "\n",
        "  seed(1)\n",
        "  #loading test data\n",
        "  X1=df_test.Bit.values.tolist()\n",
        "  Y1=df_test.Target.values.tolist()\n",
        "  y_test=Y1\n",
        "  #convert the list of strings to list of list\n",
        "  test_data = [list(map(int,k.strip(\"[]\").split(\", \"))) for k in X1]\n",
        "  #convertthe list of integer to list of list\n",
        "  test_t= extractDigits(Y1)\n",
        "  #merger elementwise the data and corresponding target\n",
        "  testset= [i + j for i, j in zip(test_data, test_t)] \n",
        "  print(\"TestSet :\\n\",testset)\n",
        "\n",
        "  y_pred=back_propagation(dataset, testset, l_rate=0.005, n_epoch=150, n_hidden=5)\n",
        "  print(y_test)\n",
        "  print(y_pred)\n",
        "  "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter :6\n",
            "Training Dataset :\n",
            "     ID                 Bit  Target\n",
            "0    0  [0, 0, 0, 0, 0, 0]       1\n",
            "2    2  [0, 0, 0, 0, 1, 0]       1\n",
            "4    4  [0, 0, 0, 1, 0, 0]       1\n",
            "6    6  [0, 0, 0, 1, 1, 0]       1\n",
            "8    8  [0, 0, 1, 0, 0, 0]       1\n",
            "10  10  [0, 0, 1, 0, 1, 0]       1\n",
            "12  12  [0, 0, 1, 1, 0, 0]       1\n",
            "14  14  [0, 0, 1, 1, 1, 0]       1\n",
            "16  16  [0, 1, 0, 0, 0, 0]       1\n",
            "18  18  [0, 1, 0, 0, 1, 0]       1\n",
            "20  20  [0, 1, 0, 1, 0, 0]       1\n",
            "22  22  [0, 1, 0, 1, 1, 0]       1\n",
            "24  24  [0, 1, 1, 0, 0, 0]       1\n",
            "26  26  [0, 1, 1, 0, 1, 0]       1\n",
            "28  28  [0, 1, 1, 1, 0, 0]       1\n",
            "30  30  [0, 1, 1, 1, 1, 0]       1\n",
            "32  32  [1, 0, 0, 0, 0, 0]       0\n",
            "34  34  [1, 0, 0, 0, 1, 0]       0\n",
            "36  36  [1, 0, 0, 1, 0, 0]       0\n",
            "38  38  [1, 0, 0, 1, 1, 0]       0\n",
            "40  40  [1, 0, 1, 0, 0, 0]       0\n",
            "42  42  [1, 0, 1, 0, 1, 0]       0\n",
            "44  44  [1, 0, 1, 1, 0, 0]       0\n",
            "46  46  [1, 0, 1, 1, 1, 0]       0\n",
            "48  48  [1, 1, 0, 0, 0, 0]       0\n",
            "50  50  [1, 1, 0, 0, 1, 0]       0\n",
            "52  52  [1, 1, 0, 1, 0, 0]       0\n",
            "54  54  [1, 1, 0, 1, 1, 0]       0\n",
            "56  56  [1, 1, 1, 0, 0, 0]       0\n",
            "58  58  [1, 1, 1, 0, 1, 0]       0\n",
            "60  60  [1, 1, 1, 1, 0, 0]       0\n",
            "62  62  [1, 1, 1, 1, 1, 0]       0\n",
            "Testing Dataset :\n",
            "     ID                 Bit  Target\n",
            "1    1  [0, 0, 0, 0, 0, 1]       1\n",
            "3    3  [0, 0, 0, 0, 1, 1]       1\n",
            "5    5  [0, 0, 0, 1, 0, 1]       1\n",
            "7    7  [0, 0, 0, 1, 1, 1]       1\n",
            "9    9  [0, 0, 1, 0, 0, 1]       1\n",
            "11  11  [0, 0, 1, 0, 1, 1]       1\n",
            "13  13  [0, 0, 1, 1, 0, 1]       1\n",
            "15  15  [0, 0, 1, 1, 1, 1]       1\n",
            "17  17  [0, 1, 0, 0, 0, 1]       1\n",
            "19  19  [0, 1, 0, 0, 1, 1]       1\n",
            "21  21  [0, 1, 0, 1, 0, 1]       1\n",
            "23  23  [0, 1, 0, 1, 1, 1]       1\n",
            "25  25  [0, 1, 1, 0, 0, 1]       1\n",
            "27  27  [0, 1, 1, 0, 1, 1]       1\n",
            "29  29  [0, 1, 1, 1, 0, 1]       1\n",
            "31  31  [0, 1, 1, 1, 1, 1]       1\n",
            "33  33  [1, 0, 0, 0, 0, 1]       0\n",
            "35  35  [1, 0, 0, 0, 1, 1]       0\n",
            "37  37  [1, 0, 0, 1, 0, 1]       0\n",
            "39  39  [1, 0, 0, 1, 1, 1]       0\n",
            "41  41  [1, 0, 1, 0, 0, 1]       0\n",
            "43  43  [1, 0, 1, 0, 1, 1]       0\n",
            "45  45  [1, 0, 1, 1, 0, 1]       0\n",
            "47  47  [1, 0, 1, 1, 1, 1]       0\n",
            "49  49  [1, 1, 0, 0, 0, 1]       0\n",
            "51  51  [1, 1, 0, 0, 1, 1]       0\n",
            "53  53  [1, 1, 0, 1, 0, 1]       0\n",
            "55  55  [1, 1, 0, 1, 1, 1]       0\n",
            "57  57  [1, 1, 1, 0, 0, 1]       0\n",
            "59  59  [1, 1, 1, 0, 1, 1]       0\n",
            "61  61  [1, 1, 1, 1, 0, 1]       0\n",
            "63  63  [1, 1, 1, 1, 1, 1]       0\n",
            "TrainSet :\n",
            " [[0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 1, 1, 0, 1], [0, 0, 1, 0, 0, 0, 1], [0, 0, 1, 0, 1, 0, 1], [0, 0, 1, 1, 0, 0, 1], [0, 0, 1, 1, 1, 0, 1], [0, 1, 0, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 1], [0, 1, 0, 1, 0, 0, 1], [0, 1, 0, 1, 1, 0, 1], [0, 1, 1, 0, 0, 0, 1], [0, 1, 1, 0, 1, 0, 1], [0, 1, 1, 1, 0, 0, 1], [0, 1, 1, 1, 1, 0, 1], [1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0, 0], [1, 0, 1, 0, 1, 0, 0], [1, 0, 1, 1, 0, 0, 0], [1, 0, 1, 1, 1, 0, 0], [1, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 0, 0], [1, 1, 0, 1, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0], [1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 0, 1, 0, 0], [1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0]]\n",
            "TestSet :\n",
            " [[0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 0, 1, 1], [0, 0, 0, 1, 1, 1, 1], [0, 0, 1, 0, 0, 1, 1], [0, 0, 1, 0, 1, 1, 1], [0, 0, 1, 1, 0, 1, 1], [0, 0, 1, 1, 1, 1, 1], [0, 1, 0, 0, 0, 1, 1], [0, 1, 0, 0, 1, 1, 1], [0, 1, 0, 1, 0, 1, 1], [0, 1, 0, 1, 1, 1, 1], [0, 1, 1, 0, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1], [0, 1, 1, 1, 0, 1, 1], [0, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 1, 1, 0], [1, 0, 0, 1, 0, 1, 0], [1, 0, 0, 1, 1, 1, 0], [1, 0, 1, 0, 0, 1, 0], [1, 0, 1, 0, 1, 1, 0], [1, 0, 1, 1, 0, 1, 0], [1, 0, 1, 1, 1, 1, 0], [1, 1, 0, 0, 0, 1, 0], [1, 1, 0, 0, 1, 1, 0], [1, 1, 0, 1, 0, 1, 0], [1, 1, 0, 1, 1, 1, 0], [1, 1, 1, 0, 0, 1, 0], [1, 1, 1, 0, 1, 1, 0], [1, 1, 1, 1, 0, 1, 0], [1, 1, 1, 1, 1, 1, 0]]\n",
            ">epoch=0, lrate=0.005, error=28.696\n",
            ">epoch=1, lrate=0.005, error=28.650\n",
            ">epoch=2, lrate=0.005, error=28.603\n",
            ">epoch=3, lrate=0.005, error=28.555\n",
            ">epoch=4, lrate=0.005, error=28.506\n",
            ">epoch=5, lrate=0.005, error=28.455\n",
            ">epoch=6, lrate=0.005, error=28.403\n",
            ">epoch=7, lrate=0.005, error=28.350\n",
            ">epoch=8, lrate=0.005, error=28.296\n",
            ">epoch=9, lrate=0.005, error=28.240\n",
            ">epoch=10, lrate=0.005, error=28.183\n",
            ">epoch=11, lrate=0.005, error=28.125\n",
            ">epoch=12, lrate=0.005, error=28.065\n",
            ">epoch=13, lrate=0.005, error=28.003\n",
            ">epoch=14, lrate=0.005, error=27.940\n",
            ">epoch=15, lrate=0.005, error=27.875\n",
            ">epoch=16, lrate=0.005, error=27.809\n",
            ">epoch=17, lrate=0.005, error=27.741\n",
            ">epoch=18, lrate=0.005, error=27.671\n",
            ">epoch=19, lrate=0.005, error=27.600\n",
            ">epoch=20, lrate=0.005, error=27.526\n",
            ">epoch=21, lrate=0.005, error=27.451\n",
            ">epoch=22, lrate=0.005, error=27.374\n",
            ">epoch=23, lrate=0.005, error=27.295\n",
            ">epoch=24, lrate=0.005, error=27.213\n",
            ">epoch=25, lrate=0.005, error=27.130\n",
            ">epoch=26, lrate=0.005, error=27.045\n",
            ">epoch=27, lrate=0.005, error=26.957\n",
            ">epoch=28, lrate=0.005, error=26.867\n",
            ">epoch=29, lrate=0.005, error=26.775\n",
            ">epoch=30, lrate=0.005, error=26.681\n",
            ">epoch=31, lrate=0.005, error=26.584\n",
            ">epoch=32, lrate=0.005, error=26.485\n",
            ">epoch=33, lrate=0.005, error=26.384\n",
            ">epoch=34, lrate=0.005, error=26.280\n",
            ">epoch=35, lrate=0.005, error=26.174\n",
            ">epoch=36, lrate=0.005, error=26.065\n",
            ">epoch=37, lrate=0.005, error=25.954\n",
            ">epoch=38, lrate=0.005, error=25.841\n",
            ">epoch=39, lrate=0.005, error=25.725\n",
            ">epoch=40, lrate=0.005, error=25.606\n",
            ">epoch=41, lrate=0.005, error=25.485\n",
            ">epoch=42, lrate=0.005, error=25.362\n",
            ">epoch=43, lrate=0.005, error=25.237\n",
            ">epoch=44, lrate=0.005, error=25.109\n",
            ">epoch=45, lrate=0.005, error=24.979\n",
            ">epoch=46, lrate=0.005, error=24.846\n",
            ">epoch=47, lrate=0.005, error=24.712\n",
            ">epoch=48, lrate=0.005, error=24.575\n",
            ">epoch=49, lrate=0.005, error=24.437\n",
            ">epoch=50, lrate=0.005, error=24.297\n",
            ">epoch=51, lrate=0.005, error=24.155\n",
            ">epoch=52, lrate=0.005, error=24.012\n",
            ">epoch=53, lrate=0.005, error=23.867\n",
            ">epoch=54, lrate=0.005, error=23.721\n",
            ">epoch=55, lrate=0.005, error=23.574\n",
            ">epoch=56, lrate=0.005, error=23.426\n",
            ">epoch=57, lrate=0.005, error=23.277\n",
            ">epoch=58, lrate=0.005, error=23.127\n",
            ">epoch=59, lrate=0.005, error=22.976\n",
            ">epoch=60, lrate=0.005, error=22.826\n",
            ">epoch=61, lrate=0.005, error=22.675\n",
            ">epoch=62, lrate=0.005, error=22.524\n",
            ">epoch=63, lrate=0.005, error=22.372\n",
            ">epoch=64, lrate=0.005, error=22.221\n",
            ">epoch=65, lrate=0.005, error=22.071\n",
            ">epoch=66, lrate=0.005, error=21.920\n",
            ">epoch=67, lrate=0.005, error=21.770\n",
            ">epoch=68, lrate=0.005, error=21.621\n",
            ">epoch=69, lrate=0.005, error=21.472\n",
            ">epoch=70, lrate=0.005, error=21.323\n",
            ">epoch=71, lrate=0.005, error=21.175\n",
            ">epoch=72, lrate=0.005, error=21.028\n",
            ">epoch=73, lrate=0.005, error=20.882\n",
            ">epoch=74, lrate=0.005, error=20.736\n",
            ">epoch=75, lrate=0.005, error=20.591\n",
            ">epoch=76, lrate=0.005, error=20.446\n",
            ">epoch=77, lrate=0.005, error=20.303\n",
            ">epoch=78, lrate=0.005, error=20.160\n",
            ">epoch=79, lrate=0.005, error=20.017\n",
            ">epoch=80, lrate=0.005, error=19.876\n",
            ">epoch=81, lrate=0.005, error=19.735\n",
            ">epoch=82, lrate=0.005, error=19.595\n",
            ">epoch=83, lrate=0.005, error=19.456\n",
            ">epoch=84, lrate=0.005, error=19.318\n",
            ">epoch=85, lrate=0.005, error=19.181\n",
            ">epoch=86, lrate=0.005, error=19.045\n",
            ">epoch=87, lrate=0.005, error=18.910\n",
            ">epoch=88, lrate=0.005, error=18.777\n",
            ">epoch=89, lrate=0.005, error=18.644\n",
            ">epoch=90, lrate=0.005, error=18.513\n",
            ">epoch=91, lrate=0.005, error=18.384\n",
            ">epoch=92, lrate=0.005, error=18.257\n",
            ">epoch=93, lrate=0.005, error=18.131\n",
            ">epoch=94, lrate=0.005, error=18.008\n",
            ">epoch=95, lrate=0.005, error=17.887\n",
            ">epoch=96, lrate=0.005, error=17.769\n",
            ">epoch=97, lrate=0.005, error=17.653\n",
            ">epoch=98, lrate=0.005, error=17.539\n",
            ">epoch=99, lrate=0.005, error=17.429\n",
            ">epoch=100, lrate=0.005, error=17.322\n",
            ">epoch=101, lrate=0.005, error=17.218\n",
            ">epoch=102, lrate=0.005, error=17.118\n",
            ">epoch=103, lrate=0.005, error=17.020\n",
            ">epoch=104, lrate=0.005, error=16.927\n",
            ">epoch=105, lrate=0.005, error=16.836\n",
            ">epoch=106, lrate=0.005, error=16.750\n",
            ">epoch=107, lrate=0.005, error=16.667\n",
            ">epoch=108, lrate=0.005, error=16.588\n",
            ">epoch=109, lrate=0.005, error=16.512\n",
            ">epoch=110, lrate=0.005, error=16.440\n",
            ">epoch=111, lrate=0.005, error=16.371\n",
            ">epoch=112, lrate=0.005, error=16.306\n",
            ">epoch=113, lrate=0.005, error=16.244\n",
            ">epoch=114, lrate=0.005, error=16.185\n",
            ">epoch=115, lrate=0.005, error=16.130\n",
            ">epoch=116, lrate=0.005, error=16.078\n",
            ">epoch=117, lrate=0.005, error=16.029\n",
            ">epoch=118, lrate=0.005, error=15.982\n",
            ">epoch=119, lrate=0.005, error=15.939\n",
            ">epoch=120, lrate=0.005, error=15.898\n",
            ">epoch=121, lrate=0.005, error=15.859\n",
            ">epoch=122, lrate=0.005, error=15.823\n",
            ">epoch=123, lrate=0.005, error=15.789\n",
            ">epoch=124, lrate=0.005, error=15.757\n",
            ">epoch=125, lrate=0.005, error=15.727\n",
            ">epoch=126, lrate=0.005, error=15.699\n",
            ">epoch=127, lrate=0.005, error=15.672\n",
            ">epoch=128, lrate=0.005, error=15.648\n",
            ">epoch=129, lrate=0.005, error=15.624\n",
            ">epoch=130, lrate=0.005, error=15.603\n",
            ">epoch=131, lrate=0.005, error=15.582\n",
            ">epoch=132, lrate=0.005, error=15.563\n",
            ">epoch=133, lrate=0.005, error=15.545\n",
            ">epoch=134, lrate=0.005, error=15.528\n",
            ">epoch=135, lrate=0.005, error=15.512\n",
            ">epoch=136, lrate=0.005, error=15.497\n",
            ">epoch=137, lrate=0.005, error=15.483\n",
            ">epoch=138, lrate=0.005, error=15.470\n",
            ">epoch=139, lrate=0.005, error=15.457\n",
            ">epoch=140, lrate=0.005, error=15.445\n",
            ">epoch=141, lrate=0.005, error=15.434\n",
            ">epoch=142, lrate=0.005, error=15.423\n",
            ">epoch=143, lrate=0.005, error=15.413\n",
            ">epoch=144, lrate=0.005, error=15.403\n",
            ">epoch=145, lrate=0.005, error=15.394\n",
            ">epoch=146, lrate=0.005, error=15.385\n",
            ">epoch=147, lrate=0.005, error=15.377\n",
            ">epoch=148, lrate=0.005, error=15.369\n",
            ">epoch=149, lrate=0.005, error=15.361\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pne7Qb_VM7f1"
      },
      "source": [
        "\n",
        "# **Evaluating Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Ccg4eRNAcf",
        "outputId": "8157e8b4-54af-46a2-c9a0-fd0bdec09cbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "results = confusion_matrix(y_test,y_pred) \n",
        "print ('Confusion Matrix :')\n",
        "print(results) \n",
        "print ('Accuracy Score :',accuracy_score(y_test,y_pred))\n",
        "print ('Report : ')\n",
        "print (classification_report(y_test,y_pred))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[16  0]\n",
            " [12  4]]\n",
            "Accuracy Score : 0.625\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      1.00      0.73        16\n",
            "           1       1.00      0.25      0.40        16\n",
            "\n",
            "    accuracy                           0.62        32\n",
            "   macro avg       0.79      0.62      0.56        32\n",
            "weighted avg       0.79      0.62      0.56        32\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcREilwjdAl0"
      },
      "source": [
        "# **Reference:**\n",
        "1. [https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWr-nIYL21pI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}