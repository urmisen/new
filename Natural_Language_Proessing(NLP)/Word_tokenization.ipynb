{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_tokenization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPLtpmi3Sfln"
      },
      "source": [
        "### Word tokenization with NLTK\n",
        "\n",
        "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!\n",
        "\n",
        "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.\n",
        "\n",
        "* Import the sent_tokenize and word_tokenize functions from nltk.tokenize.\n",
        "* Tokenize all the sentences in scene_one using the sent_tokenize() function.\n",
        "* Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.\n",
        "* Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().\n",
        "* Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTwG8ZQ9SabR"
      },
      "source": [
        "# Import necessary modules\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgIYRcKXWtGj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}