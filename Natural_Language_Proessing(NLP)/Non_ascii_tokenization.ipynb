{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Non-ascii_tokenization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPLtpmi3Sfln"
      },
      "source": [
        "### Non-ascii tokenization\n",
        "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
        "\n",
        "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
        "\n",
        "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
        "\n",
        "Unicode ranges for emoji are:\n",
        "\n",
        "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF').\n",
        "\n",
        "* okenize all the words in german_text using word_tokenize(), and print the result.\n",
        "* Tokenize only the capital words in german_text.\n",
        "1. First, write a pattern called capital_words to match only capital words. Make sure to check for the German Ü! To use this character in the exercise, copy and paste it from these instructions.\n",
        "2. Then, tokenize it using regexp_tokenize().\n",
        "\n",
        "* Tokenize only the emoji in german_text. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use regexp_tokenize() to tokenize the emoji."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTwG8ZQ9SabR"
      },
      "source": [
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)\n",
        "\n",
        "# Tokenize and print only capital words\n",
        "capital_words = r\"[A-ZÜ]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words))\n",
        "\n",
        "# Tokenize and print only emoji\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(german_text, emoji))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt2wOIOglPZR"
      },
      "source": [
        "* Write a new pattern called pattern2 to match mentions and hashtags. A mention is something like @DataCamp.\n",
        "\n",
        "* Then, call regexp_tokenize() with your new hashtag pattern on the last tweet in tweets and assign the result to mentions_hashtags.\n",
        "\n",
        "* You can access the last element of a list using -1 as the index, for example, tweets[-1].\n",
        "* Print mentions_hashtags (this has been done for you)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgIYRcKXWtGj"
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvKT4JSdl5nr"
      },
      "source": [
        "* Create an instance of TweetTokenizer called tknzr and use it inside a list comprehension to tokenize each tweet into a new list called all_tokens.\n",
        "* To do this, use the .tokenize() method of tknzr, with t as your iterator variable.\n",
        "* Print all_tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGx6tlJxl9lf"
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}